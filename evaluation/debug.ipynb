{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import clip\n",
    "import torch\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import glob\n",
    "import os\n",
    "import csv\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<PIL.PngImagePlugin.PngImageFile image mode=RGB size=512x512>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img = Image.open('/hy-tmp/6000_outputs/bef80974bf024801_m021mn_cf850b40_mask.png')\n",
    "img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InferenceDataset(Dataset):\n",
    "    def __init__(self, datadir, inference_dir, test_scene, clip_preprocess, seed, eval_resolution=256, img_suffix='.jpg', inpainted_suffix='_removed.png'):\n",
    "        self.inference_dir = inference_dir\n",
    "        self.datadir = datadir\n",
    "        if not datadir.endswith('/'):\n",
    "            datadir += '/'\n",
    "        self.mask_filenames = sorted(list(glob.glob(os.path.join(self.datadir, '**', '*mask*.png'), recursive=True)))\n",
    "        self.img_filenames = [fname.rsplit('_mask', 1)[0] + img_suffix for fname in self.mask_filenames]\n",
    "        self.file_names_seed1 = [os.path.join(inference_dir, os.path.splitext(fname[len(datadir):])[0] + inpainted_suffix + str(seed[0]) + '.png')\n",
    "                                for fname in self.img_filenames]\n",
    "        self.file_names_seed2 = [os.path.join(inference_dir, os.path.splitext(fname[len(datadir):])[0] + inpainted_suffix + str(seed[1]) + '.png')\n",
    "                                for fname in self.img_filenames]\n",
    "        self.file_names_seed3 = [os.path.join(inference_dir, os.path.splitext(fname[len(datadir):])[0] + inpainted_suffix + str(seed[2]) + '.png')\n",
    "                                for fname in self.img_filenames]\n",
    "        \n",
    "        self.clip_preprocess = clip_preprocess\n",
    "        self.eval_resolution = eval_resolution\n",
    "        self.test_scene = self.read_csv_to_dict(test_scene)\n",
    "        self.ids = [file_name.rsplit('/', 1)[1].rsplit('_mask.png', 1)[0] for file_name in self.mask_filenames]\n",
    "\n",
    "        self.collect_all_classes()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ids)\n",
    "\n",
    "\n",
    "    def read_csv_to_dict(self,file_path):\n",
    "        data_dict = {}\n",
    "        \n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            reader = csv.reader(file, delimiter=',')\n",
    "            header = next(reader)  # Skip header if there is one\n",
    "            for row in reader:\n",
    "                id = row[0].rsplit('.', 1)[0]\n",
    "                LabelName = row[1]\n",
    "                BoxXMin = float(row[2])\n",
    "                BoxXMax = float(row[3])\n",
    "                BoxYMin = float(row[4])\n",
    "                BoxYMax = float(row[5])\n",
    "                \n",
    "                data_dict[id] = {\n",
    "                    'LabelName': LabelName,\n",
    "                    'BoxXMin': BoxXMin,\n",
    "                    'BoxXMax': BoxXMax,\n",
    "                    'BoxYMin': BoxYMin,\n",
    "                    'BoxYMax': BoxYMax\n",
    "                }\n",
    "                \n",
    "        return data_dict\n",
    "\n",
    "    def scale_box(self, box, scale_ratio):\n",
    "        return list(map(lambda x: int(x * scale_ratio), box))\n",
    "\n",
    "    def get_cropped_boundary(self, object_bbox, image_size_orig):\n",
    "        width, height = image_size_orig\n",
    "        min_size = min(image_size_orig)\n",
    "        object_bbox[0] -= (width - min_size) // 2\n",
    "        object_bbox[1] -= (height - min_size) // 2\n",
    "        object_bbox[2] -= (width - min_size) // 2\n",
    "        object_bbox[3] -= (height - min_size) // 2\n",
    "        object_bbox = np.clip(object_bbox, 0, min_size)\n",
    "        return object_bbox\n",
    "\n",
    "    def get_scaled_boundary(self, object_bbox, scale_ratio):\n",
    "        object_bbox = np.array(self.scale_box(object_bbox, scale_ratio))\n",
    "        return object_bbox\n",
    "\n",
    "    def read_image(self, path):\n",
    "        img = Image.open(path).resize((self.eval_resolution,self.eval_resolution), Image.Resampling.BILINEAR)\n",
    "        return img\n",
    "\n",
    "    def collect_all_classes(self):\n",
    "        classes = set()\n",
    "        for scene_id in self.ids:\n",
    "            classes.add(self.test_scene[scene_id][\"LabelName\"])\n",
    "        self.classes = list(classes)\n",
    "\n",
    "    def add_padding(self, image):\t\n",
    "        padding_color = (0,0,0)\n",
    "        width, height = image.size\t\n",
    "        if width > height:\t\n",
    "            padded_image = Image.new(image.mode, (width, width), padding_color)\t\n",
    "            padded_image.paste(image, (0, (width - height) // 2))\t\n",
    "        else:\t\n",
    "            padded_image = Image.new(image.mode, (height, height), padding_color)\t\n",
    "            padded_image.paste(image, ((height - width) // 2, 0))\t\n",
    "        return padded_image\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        scene_id = self.ids[idx]\n",
    "        object_bbox = (int(self.test_scene[scene_id][\"BoxXMin\"]*self.eval_resolution),\n",
    "                       int(self.test_scene[scene_id][\"BoxYMin\"]*self.eval_resolution),\n",
    "                       int(self.test_scene[scene_id][\"BoxXMax\"]*self.eval_resolution),\n",
    "                       int(self.test_scene[scene_id][\"BoxYMax\"]*self.eval_resolution))\n",
    "        object_name = self.test_scene[scene_id][\"LabelName\"]\n",
    "\n",
    "        source_image = self.read_image(self.img_filenames[idx])\n",
    "\n",
    "        inpainted_image_seed1 = self.read_image(self.file_names_seed1[idx])\n",
    "        inpainted_image_seed2 = self.read_image(self.file_names_seed2[idx])\n",
    "        inpainted_image_seed3 = self.read_image(self.file_names_seed3[idx])\n",
    "        \n",
    "        #object_bbox = self.get_cropped_boundary(object_bbox, image_size_orig)\n",
    "        \n",
    "        #scale_ratio =  self.eval_resolution / min(image_size_orig)\n",
    "        #object_bbox = np.array(self.scale_box(object_bbox, scale_ratio))\n",
    "        return (\n",
    "            self.clip_preprocess(self.add_padding(source_image.crop(object_bbox))),\t\n",
    "            self.clip_preprocess(self.add_padding(inpainted_image_seed1.crop(object_bbox))),\n",
    "            self.clip_preprocess(self.add_padding(inpainted_image_seed2.crop(object_bbox))),\n",
    "            self.clip_preprocess(self.add_padding(inpainted_image_seed3.crop(object_bbox))),\n",
    "            object_name,\n",
    "            scene_id,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CLIPMetric:\n",
    "    def __init__(self, model_name=\"ViT-B/32\", device=\"cuda\"):\n",
    "        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        self.device = torch.device(device)\n",
    "\n",
    "        model, preprocess = clip.load(model_name, device=self.device)\n",
    "        self.model = model.eval()\n",
    "        self.preprocess = preprocess\n",
    "\n",
    "    def score(self, images, texts):\n",
    "        images = images.to(self.device)\n",
    "        \n",
    "        # 确保texts是一个list\n",
    "        if not isinstance(texts, list):\n",
    "            texts = [texts]\n",
    "\n",
    "        scores = []\n",
    "        for img, text in zip(images, texts):\n",
    "            text_tokenized = clip.tokenize(text).to(self.device)\n",
    "            img = img.unsqueeze(0).to(self.device)  # 添加批次维度\n",
    "\n",
    "            with torch.no_grad():\n",
    "                logits_per_image, logits_per_text = self.model(img, text_tokenized)\n",
    "\n",
    "            scores.append(logits_per_image.squeeze().cpu().numpy())\n",
    "        # 将每个NumPy数组转换为PyTorch张量\n",
    "        tensor_list = [torch.tensor(arr) for arr in scores]\n",
    "\n",
    "        # 将这些张量堆叠为一个新的张量\n",
    "        stacked_tensor = torch.stack(tensor_list)\n",
    "\n",
    "        # 调整形状为[4, 1]\n",
    "        final_tensor = stacked_tensor.unsqueeze(1)\n",
    "        return final_tensor\n",
    "    \n",
    "    def calculate_clip_consensus(self, images):  \n",
    "        std = []\n",
    "        for img_seed1,img_seed2,img_seed3 in zip(*images):\n",
    "            img_seeds = [img_seed1, img_seed2, img_seed3]\n",
    "            embeddings = []\n",
    "            for image in img_seeds:\n",
    "                image = image.unsqueeze(0).to(self.device)\n",
    "                with torch.no_grad():\n",
    "                    image_features = self.model.encode_image(image)\n",
    "                embeddings.append(image_features.cpu().numpy())\n",
    "\n",
    "            # 将嵌入堆叠成一个numpy数组\n",
    "            embeddings = np.vstack(embeddings)\n",
    "            \n",
    "            # 计算嵌入的标准差\n",
    "            consensus_std = np.std(embeddings, axis=0)\n",
    "            std.append(consensus_std.mean())\n",
    "        # 将每个NumPy数组转换为PyTorch张量\n",
    "        tensor_list = [torch.tensor(arr) for arr in std]\n",
    "\n",
    "        # 将这些张量堆叠为一个新的张量\n",
    "        stacked_tensor = torch.stack(tensor_list)\n",
    "\n",
    "        # 调整形状为[4, 1]\n",
    "        final_tensor = stacked_tensor.unsqueeze(1)\n",
    "        # 返回标准差的均值作为CLIP consensus指标\n",
    "        return final_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "clip_metric = CLIPMetric(model_name=\"ViT-B/32\")\n",
    "\n",
    "dataset = InferenceDataset('/hy-tmp/100_outputs', '/hy-tmp/100_outputs', '/hy-tmp/DATA/fetch_output_100.csv', \n",
    "                           clip_metric.preprocess, \n",
    "                           seed = [123,321,777],\n",
    "                           eval_resolution=512, \n",
    "                           img_suffix='.png', \n",
    "                           inpainted_suffix='_removed_')\n",
    "dataloader = DataLoader(dataset, batch_size=4, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_scores = {}\n",
    "scene_ids = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, (source_img, inpainted_image_seed1, inpainted_image_seed2, inpainted_image_seed3, object_names, scene_id) in enumerate(dataloader):\n",
    "    scene_ids.extend(list(scene_id))\n",
    "    prompts = list(map(lambda x: f\"a photo of a {x}\", object_names))\n",
    "    src_scores = clip_metric.score(source_img, prompts)\n",
    "    prd_scores_seed1  = clip_metric.score(inpainted_image_seed1, prompts)\n",
    "    prd_scores_seed2  = clip_metric.score(inpainted_image_seed2, prompts)\n",
    "    prd_scores_seed3  = clip_metric.score(inpainted_image_seed3, prompts)\n",
    "    prd_scores_mean = (prd_scores_seed1 + prd_scores_seed2 + prd_scores_seed3)/3\n",
    "    prd_clip_consensus = clip_metric.calculate_clip_consensus([inpainted_image_seed1, inpainted_image_seed2, inpainted_image_seed3])\n",
    "    for src_score, prd_seed1 , prd_seed2, prd_seed3, mean, prd_consensus, id in zip(src_scores, prd_scores_seed1, prd_scores_seed2, prd_scores_seed3, prd_scores_mean, prd_clip_consensus, scene_id):\n",
    "        inference_scores[id] = {\n",
    "            \"src_scores\": src_score,\n",
    "            \"prd_scores_seed1\": prd_seed1,\n",
    "            \"prd_scores_seed2\": prd_seed2,\n",
    "            \"prd_scores_seed3\": prd_seed3,\n",
    "            \"prd_scores_mean\": mean,\n",
    "            \"prd_clip_consensus\": prd_consensus,\n",
    "            \"my_metric\": (src_score - mean)/prd_consensus\n",
    "        }\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['019877aafb67d078_m0306r_83131dfa',\n",
       " '022c71451277faa4_m08pbxl_19b2c72c',\n",
       " '050e9d0a85a62229_m0jy4k_874c4dda',\n",
       " '080014db39bc64ea_m03c7gz_a7ecabab']"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scene_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "clip_metric = CLIPMetric(model_name=\"ViT-B/32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = Image.open('/hy-tmp/6000_outputs/0aed7fafa7ade4c7_m04yx4_ddbbf1ea.png').resize((512,512), Image.Resampling.BILINEAR)\n",
    "img_rm = Image.open('/hy-tmp/6000_outputs/0aed7fafa7ade4c7_m04yx4_ddbbf1ea_ori_123.png').resize((512,512), Image.Resampling.BILINEAR)\n",
    "img_inp = Image.open('/hy-tmp/inp_outputs_6000/0aed7fafa7ade4c7_m04yx4_ddbbf1ea_ori_123.png').resize((512,512), Image.Resampling.BILINEAR)\n",
    "img_sd = Image.open('/hy-tmp/sd_outputs_6000/0aed7fafa7ade4c7_m04yx4_ddbbf1ea_ori_123.png').resize((512,512), Image.Resampling.BILINEAR)\n",
    "img_lama = Image.open('/hy-tmp/lama_6000/0aed7fafa7ade4c7_m04yx4_ddbbf1ea_removed.png').resize((512,512), Image.Resampling.BILINEAR)\n",
    "object_name = 'Man'\n",
    "BoxXMin = 0.090147\n",
    "BoxXMax = 0.614256\n",
    "BoxYMin = 0.059098\n",
    "BoxYMax = 1.000000\n",
    "object_bbox = (int(BoxXMin*512),\n",
    "                int(BoxYMin*512),\n",
    "                int(BoxXMax*512),\n",
    "                int(BoxYMax*512))\n",
    "source_img = clip_metric.preprocess(img.crop(object_bbox)).unsqueeze(0)\n",
    "rm_img = clip_metric.preprocess(img_rm.crop(object_bbox)).unsqueeze(0)\n",
    "inp_img = clip_metric.preprocess(img_inp.crop(object_bbox)).unsqueeze(0)\n",
    "sd_img = clip_metric.preprocess(img_sd.crop(object_bbox)).unsqueeze(0)\n",
    "lama_img = clip_metric.preprocess(img_lama.crop(object_bbox)).unsqueeze(0)\n",
    "prompt = f\"A photo of a {object_name}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "source:25.59375\n",
      "rm_prompt:24.125\n",
      "rm_bk:23.421875\n",
      "lama_prompt:21.765625\n",
      "lama_bk:22.984375\n",
      "inp_prompt:22.09375\n",
      "inp_bk:21.671875\n",
      "sd_prompt:23.25\n",
      "sd_bk:21.5625\n"
     ]
    }
   ],
   "source": [
    "src_scores = clip_metric.score(source_img, prompt)\n",
    "print(f\"source:{src_scores.item()}\")\n",
    "#w_scores = clip_metric.score(source_img, \"A photo of a backgroud\")\n",
    "#print(w_scores) \n",
    "prd_scores = clip_metric.score(rm_img, prompt)\n",
    "print(f\"rm_prompt:{prd_scores.item()}\")\n",
    "prd_w_scores = clip_metric.score(rm_img, \"backgroud\")\n",
    "print(f\"rm_bk:{prd_w_scores.item()}\")\n",
    "prd_scores_lama = clip_metric.score(lama_img, prompt)\n",
    "print(f\"lama_prompt:{prd_scores_lama.item()}\")\n",
    "prd_w_scores_lama = clip_metric.score(lama_img, \"backgroud\")\n",
    "print(f\"lama_bk:{prd_w_scores_lama.item()}\")\n",
    "prd_scores_inp = clip_metric.score(inp_img, prompt)\n",
    "print(f\"inp_prompt:{prd_scores_inp.item()}\")\n",
    "prd_w_scores_inp = clip_metric.score(inp_img, \"backgroud\")\n",
    "print(f\"inp_bk:{prd_w_scores_inp.item()}\")\n",
    "prd_scores_sd = clip_metric.score(sd_img, prompt)\n",
    "print(f\"sd_prompt:{prd_scores_sd.item()}\")\n",
    "prd_w_scores_sd = clip_metric.score(sd_img, \"backgroud\")\n",
    "print(f\"sd_bk:{prd_w_scores_sd.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "source:25.59375\n",
      "rm_prompt:24.125\n",
      "rm_bk:24.609375\n",
      "inp_prompt:22.09375\n",
      "inp_bk:22.890625\n",
      "sd_prompt:23.25\n",
      "sd_bk:22.515625\n"
     ]
    }
   ],
   "source": [
    "src_scores = clip_metric.score(source_img, prompt)\n",
    "print(f\"source:{src_scores.item()}\")\n",
    "#w_scores = clip_metric.score(source_img, \"A photo of a backgroud\")\n",
    "#print(w_scores) \n",
    "prd_scores = clip_metric.score(rm_img, prompt)\n",
    "print(f\"rm_prompt:{prd_scores.item()}\")\n",
    "prd_w_scores = clip_metric.score(rm_img, \"A photo of a backgroud\")\n",
    "print(f\"rm_bk:{prd_w_scores.item()}\")\n",
    "prd_scores_inp = clip_metric.score(inp_img, prompt)\n",
    "print(f\"inp_prompt:{prd_scores_inp.item()}\")\n",
    "prd_w_scores_inp = clip_metric.score(inp_img, \"A photo of a backgroud\")\n",
    "print(f\"inp_bk:{prd_w_scores_inp.item()}\")\n",
    "prd_scores_sd = clip_metric.score(sd_img, prompt)\n",
    "print(f\"sd_prompt:{prd_scores_sd.item()}\")\n",
    "prd_w_scores_sd = clip_metric.score(sd_img, \"A photo of a backgroud\")\n",
    "print(f\"sd_bk:{prd_w_scores_sd.item()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# clip_consensus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import clip\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "def calculate_clip_consensus(image_paths):\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    \n",
    "    # 加载CLIP模型和预处理函数\n",
    "    model, preprocess = clip.load(\"ViT-B/32\", device=device)\n",
    "\n",
    "    embeddings = []\n",
    "    scores = []\n",
    "    BoxXMin = 0.026608\n",
    "    BoxXMax = 0.895787\n",
    "    BoxYMin = 0.171806\n",
    "    BoxYMax = 1.000000\n",
    "    object_bbox = (int(BoxXMin*512),\n",
    "                    int(BoxYMin*512),\n",
    "                    int(BoxXMax*512),\n",
    "                    int(BoxYMax*512))\n",
    "    prompt = \"A photo of a women\"\n",
    "    for image_path in image_paths:\n",
    "        # 预处理图像\n",
    "        image = preprocess(Image.open(image_path).crop(object_bbox)).unsqueeze(0).to(device)\n",
    "        text = clip.tokenize(prompt).to(device)\n",
    "        \n",
    "        # 计算图像嵌入\n",
    "        with torch.no_grad():\n",
    "            image_features = model.encode_image(image)\n",
    "            logits_per_image, logits_per_text = model(image, text)\n",
    "        embeddings.append(image_features.cpu().numpy())\n",
    "        scores.append(logits_per_image.cpu().numpy())\n",
    "    # 将嵌入堆叠成一个numpy数组\n",
    "    embeddings = np.vstack(embeddings)\n",
    "    scores_np = np.vstack(scores)\n",
    "    \n",
    "    # 计算嵌入的标准差\n",
    "    consensus_std = np.std(embeddings, axis=0)\n",
    "    #score_mean = np.mean(scores_np, axis=0)\n",
    "    # 返回标准差的均值作为CLIP consensus指标\n",
    "    return consensus_std.mean(),scores,scores_np.mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CLIP Consensus Score: 0.0694580078125\n",
      "CLIP Scores: [array([[21.66]], dtype=float16), array([[23.05]], dtype=float16), array([[22.45]], dtype=float16)]\n",
      "CLIP Scores Mean: 22.390625\n",
      "m:322.25\n"
     ]
    }
   ],
   "source": [
    "# 示例图片路径\n",
    "image_paths = [\"/hy-tmp/100_outputs/0a9fd00665d365a8_m03bt1vf_15f4a879_removed_123.png\", \n",
    "               \"/hy-tmp/100_outputs/0a9fd00665d365a8_m03bt1vf_15f4a879_removed_321.png\", \n",
    "               \"/hy-tmp/100_outputs/0a9fd00665d365a8_m03bt1vf_15f4a879_removed_777.png\"]\n",
    "\n",
    "# 计算CLIP consensus指标\n",
    "consensus_score,scores,scores_mean = calculate_clip_consensus(image_paths)\n",
    "print(f\"CLIP Consensus Score: {consensus_score}\")\n",
    "print(f\"CLIP Scores: {scores}\")\n",
    "print(f\"CLIP Scores Mean: {scores_mean}\")\n",
    "print(f\"m:{scores_mean/consensus_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CLIP Consensus Score: 0.1376953125\n",
      "CLIP Scores: [array([[21.36]], dtype=float16), array([[25.94]], dtype=float16), array([[25.34]], dtype=float16)]\n",
      "CLIP Scores Mean: 24.21875\n",
      "m:175.875\n"
     ]
    }
   ],
   "source": [
    "# 示例图片路径\n",
    "image_paths = [\"/hy-tmp/inp_outputs_100/0a9fd00665d365a8_m03bt1vf_15f4a879_inp_123.png\", \n",
    "               \"/hy-tmp/inp_outputs_100/0a9fd00665d365a8_m03bt1vf_15f4a879_inp_321.png\", \n",
    "               \"/hy-tmp/inp_outputs_100/0a9fd00665d365a8_m03bt1vf_15f4a879_inp_777.png\"]\n",
    "\n",
    "# 计算CLIP consensus指标\n",
    "consensus_score,scores,scores_mean = calculate_clip_consensus(image_paths)\n",
    "print(f\"CLIP Consensus Score: {consensus_score}\")\n",
    "print(f\"CLIP Scores: {scores}\")\n",
    "print(f\"CLIP Scores Mean: {scores_mean}\")\n",
    "print(f\"m:{scores_mean/consensus_score}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CLIP Consensus Score: 0.181640625\n",
      "CLIP Scores: [array([[20.16]], dtype=float16), array([[20.72]], dtype=float16), array([[24.14]], dtype=float16)]\n",
      "CLIP Scores Mean: 21.671875\n",
      "m:119.3125\n"
     ]
    }
   ],
   "source": [
    "# 示例图片路径\n",
    "image_paths = [\"/hy-tmp/sd_outputs_100/0a9fd00665d365a8_m03bt1vf_15f4a879_inp_123.png\", \n",
    "               \"/hy-tmp/sd_outputs_100/0a9fd00665d365a8_m03bt1vf_15f4a879_inp_321.png\", \n",
    "               \"/hy-tmp/sd_outputs_100/0a9fd00665d365a8_m03bt1vf_15f4a879_inp_777.png\"]\n",
    "\n",
    "# 计算CLIP consensus指标\n",
    "consensus_score,scores,scores_mean = calculate_clip_consensus(image_paths)\n",
    "print(f\"CLIP Consensus Score: {consensus_score}\")\n",
    "print(f\"CLIP Scores: {scores}\")\n",
    "print(f\"CLIP Scores Mean: {scores_mean}\")\n",
    "print(f\"m:{scores_mean/consensus_score}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
