{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MasaCtrl: Tuning-free Mutual Self-Attention Control for Consistent Image Synthesis and Editing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision.io import read_image, ImageReadMode\n",
    "import numpy as np\n",
    "\n",
    "from diffusers import DDIMScheduler, StableDiffusionPipeline\n",
    "from diffusers.utils.torch_utils import randn_tensor\n",
    "from torchvision.transforms.functional import gaussian_blur\n",
    "import cv2\n",
    "from torchvision.utils import save_image\n",
    "from torchvision.io import read_image\n",
    "from pytorch_lightning import seed_everything\n",
    "from matplotlib import pyplot as plt\n",
    "torch.cuda.set_device(0)  # set the GPU device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Switch to \"AttentiveEraser\" dictionary\n",
    "current_dir = os.getcwd()\n",
    "parent_dir = os.path.dirname(current_dir)\n",
    "os.chdir(parent_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtype = torch.float16\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "#model_path = \"stabilityai/stable-diffusion-2-1-base\"\n",
    "model_path = \"/hy-tmp/stable-diffusion-2-1-base\" #\"stable-diffusion-v1-5\" \"solarsync_v11\"/ change this to the path of the model if you are loading the model offline\n",
    "   \n",
    "scheduler = DDIMScheduler(beta_start=0.00085, beta_end=0.012, beta_schedule=\"scaled_linear\", clip_sample=False, set_alpha_to_one=False)\n",
    "pipe = StableDiffusionPipeline.from_pretrained(\n",
    "    model_path,\n",
    "    scheduler=scheduler,\n",
    "    custom_pipeline=\"./pipelines/pipeline_inversion.py\",\n",
    "    torch_dtype=dtype\n",
    ")\n",
    "pipe = pipe.to(device)\n",
    "pipe.enable_attention_slicing()\n",
    "pipe.enable_model_cpu_offload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#freeu can further improve results in some cases(https://github.com/ChenyangSi/FreeU)\n",
    "from utils import register_free_upblock2d, register_free_crossattn_upblock2d\n",
    "register_free_upblock2d(pipe, b1=1.4, b2=1.6, s1=0.9, s2=0.2) #2.1\n",
    "register_free_crossattn_upblock2d(pipe, b1=1.4, b2=1.6, s1=0.9, s2=0.2)#2.1\n",
    "#register_free_upblock2d(pipe, b1=1.5, b2=1.6, s1=0.9, s2=0.2) #1.5\n",
    "#register_free_crossattn_upblock2d(pipe, b1=1.5, b2=1.6, s1=0.9, s2=0.2) #1.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image(image_path, device):\n",
    "    image = read_image(image_path)\n",
    "    image = image[:3].unsqueeze_(0).float() / 127.5 - 1.  # [-1, 1]\n",
    "    if image.shape[1] != 3:\n",
    "        image = image.expand(-1, 3, -1, -1)\n",
    "    image = F.interpolate(image, (512, 512), mode=\"bicubic\")\n",
    "    image = image.to(dtype).to(device)\n",
    "    return image\n",
    "\n",
    "def load_mask(mask_path, device):\n",
    "    mask = read_image(mask_path,mode=ImageReadMode.GRAY)\n",
    "    mask = mask.unsqueeze_(0).float() / 255.  # 0 or 1\n",
    "    mask = F.interpolate(mask, (512, 512), mode=\"bicubic\")\n",
    "    mask = gaussian_blur(mask, kernel_size=(7,7))\n",
    "    mask[mask < 0.1] = 0\n",
    "    mask[mask >= 0.1] = 1\n",
    "    mask = mask.to(dtype).to(device)\n",
    "    return mask\n",
    "\n",
    "seed = 123\n",
    "seed_everything(seed)\n",
    "generator=torch.Generator(\"cuda\").manual_seed(seed)\n",
    "sample = \"an\" \n",
    "out_dir = f\"./workdir_inp/{sample}/\"\n",
    "os.makedirs(out_dir, exist_ok=True)\n",
    "sample_count = len(os.listdir(out_dir))\n",
    "out_dir = os.path.join(out_dir, f\"sample_{sample_count}\")\n",
    "os.makedirs(out_dir, exist_ok=True)\n",
    "# source image\n",
    "SOURCE_IMAGE_PATH = f\"./examples/img/{sample}.png\"\n",
    "MASK_PATH = f\"./examples/mask/{sample}_mask.png\"\n",
    "prompt = \"\"\n",
    "source_image = load_image(SOURCE_IMAGE_PATH, device)\n",
    "mask_an = load_mask(MASK_PATH, device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_prompt = \"\"\n",
    "target_prompt = \"\"\n",
    "with torch.no_grad():\n",
    "    # invert the source image\n",
    "    start_code, latents_list = pipe.invert(\n",
    "                                source_image,\n",
    "                                source_prompt,\n",
    "                                generator=generator,\n",
    "                                guidance_scale=1,\n",
    "                                num_inference_steps=40,\n",
    "                                return_intermediates=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from AAS.AAS import AAS\n",
    "from AAS.AAS_utils import regiter_attention_editor_diffusers\n",
    "\n",
    "strength = 0.8\n",
    "num_inference_steps = 50\n",
    "START_STEP = 0\n",
    "END_STEP = int(strength*num_inference_steps)\n",
    "LAYER = 7 # 0~5down,6mid,7~15up /layer that starting AAS\n",
    "END_LAYER = 16 # layer that ending AAS\n",
    "\n",
    "attentionstore = None\n",
    "#removelist=[6]\n",
    "layer_idx=list(range(LAYER, END_LAYER))\n",
    "ss_steps = 9 # similarity suppression steps\n",
    "ss_scale = 0.3 # similarity suppression scale\n",
    "\n",
    "editor = AAS(attentionstore, START_STEP, END_STEP, LAYER, END_LAYER,layer_idx= layer_idx, mask=mask_an, ss_steps=ss_steps,ss_scale=ss_scale)\n",
    "regiter_attention_editor_diffusers(pipe, editor)\n",
    "\n",
    "removal_guidance_scale = 9 # removal guidance scale\n",
    "\n",
    "image = pipe(\n",
    "    target_prompt,\n",
    "    width=512,\n",
    "    height=512,\n",
    "    num_inference_steps=40,\n",
    "    guidance_scale=1.0,\n",
    "    latents=start_code,\n",
    "    generator=generator,\n",
    "    x0_latents=latents_list[0],\n",
    "    record_list = list(reversed(latents_list)),\n",
    "    mask = mask_an,\n",
    "    removal_guidance_scale = removal_guidance_scale, \n",
    "    return_intermediates = False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_redder(img, mask, increase_factor=0.25):\n",
    "    img_redder = img.clone()\n",
    "    mask_expanded = mask.expand_as(img)\n",
    "    img_redder[0][mask_expanded[0] == 1] = torch.clamp(img_redder[0][mask_expanded[0] == 1] + increase_factor, 0, 1)\n",
    "    \n",
    "    return img_redder\n",
    "img = (source_image* 0.5 + 0.5).squeeze(0)\n",
    "mask = mask_an.squeeze(0)\n",
    "img_redder = make_redder(img, mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.transforms.functional import to_pil_image, to_tensor\n",
    "from PIL import ImageFilter\n",
    "pil_mask = to_pil_image(mask.squeeze(0))\n",
    "pil_mask_blurred = pil_mask.filter(ImageFilter.GaussianBlur(radius=15))\n",
    "mask_blurred = to_tensor(pil_mask_blurred).unsqueeze_(0).to(mask.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_tile = mask * image[-1:] + (1 - mask) * (source_image* 0.5 + 0.5)\n",
    "out_tile_1 = mask_blurred * image[-1:] + (1 - mask_blurred) * out_tile\n",
    "#out_image = torch.concat([source_image* 0.5 + 0.5,\n",
    "out_image = torch.concat([img_redder.unsqueeze(0),\n",
    "                         image[-1:],\n",
    "                         out_tile_1],\n",
    "                         #image[:1]],\n",
    "                         dim=0)\n",
    "save_image(out_image, os.path.join(out_dir, f\"all_step{END_STEP}_layer{LAYER}.png\"))\n",
    "save_image(out_image[0], os.path.join(out_dir, f\"source_step{END_STEP}_layer{LAYER}.png\"))\n",
    "save_image(out_image[1], os.path.join(out_dir, f\"AE_step{END_STEP}_layer{LAYER}.png\"))\n",
    "save_image(out_image[2], os.path.join(out_dir, f\"AE_tile_step{END_STEP}_layer{LAYER}.png\"))\n",
    "#save_image(out_image[2], os.path.join(out_dir, f\"compare_step{END_STEP}_layer{LAYER}.png\"))\n",
    "print(\"Syntheiszed images are saved in\", out_dir)\n",
    "img_ori = cv2.imread(os.path.join(out_dir, f\"all_step{END_STEP}_layer{LAYER}.png\"))\n",
    "img_ori = cv2.cvtColor(img_ori, cv2.COLOR_BGR2RGB)\n",
    "plt.figure(figsize=(18, 24))\n",
    "plt.imshow(img_ori)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AE",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
