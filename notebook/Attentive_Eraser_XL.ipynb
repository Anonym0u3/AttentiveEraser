{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from diffusers import DDIMScheduler,StableDiffusionXLInpaintPipeline\n",
    "from torchvision.io import read_image, ImageReadMode\n",
    "import torch.nn.functional as F\n",
    "import cv2\n",
    "from torchvision.utils import save_image\n",
    "from torchvision.io import read_image\n",
    "from torchvision.transforms.functional import gaussian_blur\n",
    "from matplotlib import pyplot as plt\n",
    "torch.cuda.set_device(0)  # set the GPU device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Switch to \"AttentiveEraser\" dictionary\n",
    "current_dir = os.getcwd()\n",
    "parent_dir = os.path.dirname(current_dir)\n",
    "os.chdir(parent_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtype = torch.float16\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "scheduler = DDIMScheduler(beta_start=0.00085, beta_end=0.012, beta_schedule=\"scaled_linear\", clip_sample=False, set_alpha_to_one=False)\n",
    "\n",
    "#model_path = \"stabilityai/stable-diffusion-xl-base-1.0\"\n",
    "model_path = \"/hy-tmp/stable-diffusion-xl-base-1.0\" # change this to the path of the model if you are loading the model offline\n",
    "\n",
    "base = StableDiffusionXLInpaintPipeline.from_pretrained(\n",
    "    model_path,\n",
    "    custom_pipeline=\"./pipelines/SDXL_inp_pipeline.py\",\n",
    "    scheduler=scheduler,\n",
    "    variant=\"fp16\",\n",
    "    use_safetensors=True,\n",
    "    torch_dtype=dtype,\n",
    ").to(device)\n",
    "base.enable_attention_slicing()\n",
    "base.enable_model_cpu_offload()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" #freeu can further improve results in some cases(https://github.com/ChenyangSi/FreeU)\n",
    "from utils import register_free_upblock2d, register_free_crossattn_upblock2d\n",
    "register_free_upblock2d(base, b1=1.3, b2=1.4, s1=0.9, s2=0.2)\n",
    "register_free_crossattn_upblock2d(base, b1=1.3, b2=1.4, s1=0.9, s2=0.2) \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed=123 \n",
    "g = torch.Generator('cuda').manual_seed(seed)\n",
    "def load_image(image_path, device):\n",
    "    image = read_image(image_path)\n",
    "    image = image[:3].unsqueeze_(0).float() / 127.5 - 1.  # [-1, 1]\n",
    "    if image.shape[1] != 3:\n",
    "        image = image.expand(-1, 3, -1, -1)\n",
    "    image = F.interpolate(image, (1024, 1024))\n",
    "    image = image.to(dtype).to(device)\n",
    "    return image\n",
    "\n",
    "def load_mask(mask_path, device):\n",
    "    mask = read_image(mask_path,mode=ImageReadMode.GRAY)\n",
    "    mask = mask.unsqueeze_(0).float() / 255.  # 0 or 1\n",
    "    mask = F.interpolate(mask, (1024, 1024))\n",
    "    mask = gaussian_blur(mask, kernel_size=(77, 77))\n",
    "    mask[mask < 0.1] = 0\n",
    "    mask[mask >= 0.1] = 1\n",
    "    mask = mask.to(dtype).to(device)\n",
    "    return mask\n",
    "\n",
    "\n",
    "sample = \"an1024\"\n",
    "prompt = \"\"\n",
    "out_dir = f\"./workdir_xl/{sample}/\"\n",
    "os.makedirs(out_dir, exist_ok=True)\n",
    "sample_count = len(os.listdir(out_dir))\n",
    "out_dir = os.path.join(out_dir, f\"sample_{sample_count}\")\n",
    "os.makedirs(out_dir, exist_ok=True)\n",
    "SOURCE_IMAGE_PATH = f\"./examples/img/{sample}.png\"\n",
    "MASK_PATH = f\"./examples/mask/{sample}_mask.png\"\n",
    "\n",
    "\n",
    "source_image = load_image(SOURCE_IMAGE_PATH, device)\n",
    "mask_an = load_mask(MASK_PATH, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from AAS.AAS import AAS_XL\n",
    "from AAS.AAS_utils import regiter_attention_editor_diffusers\n",
    "strength = 0.8\n",
    "num_inference_steps = 50\n",
    "START_STEP = 0\n",
    "END_STEP = int(strength*num_inference_steps)\n",
    "LAYER = 34 # 0~23down,24~33mid,34~69up /layer that starting AAS\n",
    "END_LAYER = 70 # layer that ending AAS\n",
    "layer_idx=list(range(LAYER, END_LAYER))\n",
    "ss_steps = 9 # similarity suppression steps\n",
    "ss_scale = 0.3 # similarity suppression scale\n",
    "# hijack the attention module\n",
    "editor = AAS_XL(START_STEP, END_STEP, LAYER, END_LAYER,layer_idx= layer_idx, mask=mask_an,model_type=\"SDXL\",ss_steps=ss_steps,ss_scale=ss_scale)\n",
    "regiter_attention_editor_diffusers(base, editor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rm_guidance_scale = 9 # removal guidance scale\n",
    "image = base(\n",
    "    prompt=prompt,\n",
    "    image=source_image,\n",
    "    height=1024,\n",
    "    width=1024,\n",
    "    rm_guidance_scale=rm_guidance_scale,\n",
    "    strength=strength,\n",
    "    mask_image=mask_an,\n",
    "    generator=g,\n",
    "    num_inference_steps=num_inference_steps,\n",
    "    guidance_scale=1,\n",
    "    output_type='pt'\n",
    ").images[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_redder(img, mask, increase_factor=0.4):\n",
    "    img_redder = img.clone()\n",
    "    mask_expanded = mask.expand_as(img)\n",
    "    img_redder[0][mask_expanded[0] == 1] = torch.clamp(img_redder[0][mask_expanded[0] == 1] + increase_factor, 0, 1)\n",
    "    \n",
    "    return img_redder\n",
    "img = (source_image* 0.5 + 0.5).squeeze(0)\n",
    "mask_red = mask_an.squeeze(0)\n",
    "img_redder = make_redder(img, mask_red)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.transforms.functional import to_pil_image, to_tensor\n",
    "from PIL import ImageFilter\n",
    "pil_mask = to_pil_image(mask_an.squeeze(0))\n",
    "pil_mask_blurred = pil_mask.filter(ImageFilter.GaussianBlur(radius=15))\n",
    "mask_blurred = to_tensor(pil_mask_blurred).unsqueeze_(0).to(mask_an.device)\n",
    "msak_f = 1-(1-mask_an)*(1-mask_blurred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_1=image.unsqueeze(0)\n",
    "out_tile = msak_f * image_1 + (1 - msak_f) * (source_image* 0.5 + 0.5)\n",
    "out_image = torch.concat([img_redder.unsqueeze(0),\n",
    "                         image_1,\n",
    "                         out_tile],\n",
    "                         dim=0)\n",
    "save_image(out_image, os.path.join(out_dir, f\"all_step{END_STEP}_layer{LAYER}.png\"))\n",
    "save_image(out_image[0], os.path.join(out_dir, f\"source_step{END_STEP}_layer{LAYER}.png\"))\n",
    "save_image(out_image[1], os.path.join(out_dir, f\"AE_step{END_STEP}_layer{LAYER}.png\"))\n",
    "save_image(out_image[2], os.path.join(out_dir, f\"AE_tile_step{END_STEP}_layer{LAYER}.png\"))\n",
    "print(\"Syntheiszed images are saved in\", out_dir)\n",
    "img_ori = cv2.imread(os.path.join(out_dir, f\"all_step{END_STEP}_layer{LAYER}.png\"))\n",
    "img_ori = cv2.cvtColor(img_ori, cv2.COLOR_BGR2RGB)\n",
    "plt.figure(figsize=(20, 26))\n",
    "plt.imshow(img_ori)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AE",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
